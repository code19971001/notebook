## Spark 内核

### 1 环境准备--Yarn

command：

```shell
E:\bigdata\spark-3.0.0-bin-hadoop3.2>bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[2] examples/jars/spark-examples_2.12-3.0.0.jar
```

调用链：`spark-submit.cmd`->`spark-submit2.cmd`->`spark-class2.cmd`

```shell
del %LAUNCHER_OUTPUT%
# 打印出最终执行的命令
echo %SPARK_CMD%
%SPARK_CMD%
```

![image-20220304133747607](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20220304133747607.png)

实际上使用`java org.apache.spark.deploy.SparkSubmit`启动了一个进程，并执行他的`main`方法，因此我们可以查看这个类的源码。

```scala
override def main(args: Array[String]): Unit = {
  val submit = new SparkSubmit() {
    self =>

    override protected def parseArguments(args: Array[String]): SparkSubmitArguments = {
      new SparkSubmitArguments(args) {
        override protected def logInfo(msg: => String): Unit = self.logInfo(msg)

        override protected def logWarning(msg: => String): Unit = self.logWarning(msg)

        override protected def logError(msg: => String): Unit = self.logError(msg)
      }
    }

    override protected def logInfo(msg: => String): Unit = printMessage(msg)

    override protected def logWarning(msg: => String): Unit = printMessage(s"Warning: $msg")

    override protected def logError(msg: => String): Unit = printMessage(s"Error: $msg")

    override def doSubmit(args: Array[String]): Unit = {
      try {
        super.doSubmit(args)
      } catch {
        case e: SparkUserAppException =>
          exitFn(e.exitCode)
      }
    }

  }

  submit.doSubmit(args)
}
```

类：`org.apache.spark.deploy.yarn.YarnClusterApplication`

package：

```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-yarn_2.12</artifactId>
    <version>3.0.0</version>
</dependency>
```



### 2 组件通信

> 使用的通信框架是Netty，AIO的方式。windows支持的比较好，Linux对AIO支持不够好，所以在Linux上使用Epoll的方式模仿AIO。

- BIO：阻塞式IO。
- NIO：非阻塞式IO。
- AIO：异步非阻塞式IO。

![image-20220307231412729](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20220307231412729.png)

Spark2.x基于netty，使用actor的方式实现组件内的通信，一个节点对应一个inbox已经多个Outbox。

![image-20220307233026030](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20220307233026030.png)

### 3 应用程序的执行

#### 3.1 RDD的依赖关系

#### 3.2 阶段的划分

#### 3.3 任务的划分

#### 3.4 任务的调度

#### 3.5 任务的执行

### 4 Shuffle

#### 4.1 shuffle的原理和执行过程

![image-20220315221045397](C:\Users\code1997\AppData\Roaming\Typora\typora-user-images\image-20220315221045397.png)

#### 4.2 Shuffle写磁盘

#### 4.3 shuffle读取磁盘

ShuffledRDD

```scala
  override def compute(split: Partition, context: TaskContext): Iterator[(K, C)] = {
    val dep = dependencies.head.asInstanceOf[ShuffleDependency[K, V, C]]
    val metrics = context.taskMetrics().createTempShuffleReadMetrics()
    SparkEnv.get.shuffleManager.getReader(
      dep.shuffleHandle, split.index, split.index + 1, context, metrics)
      .read()
      .asInstanceOf[Iterator[(K, C)]]
  }
```

ShuffleWriteProcessor：shuffle处理器

- Hash(早期版本)

- SortShuffleManager
  
  | 处理器                       | 写对象                | 判断条件                                                     |
  | ---------------------------- | --------------------- | ------------------------------------------------------------ |
  | SerializedShuffleHandle      | unsafeShuffleHandle   | 1.序列化规则支持重定位操作（java不支持，kyro支持）.<br />2.不能使用预聚合<br />3.如果下游的分区数量小于或等于16777216 |
  | BypassMergeSortShuffleHandle | bypassMergeSortHandle | 1.不能使用预聚合<br />2.如果校友的分区数量小于等于200(可配置) |
  | BaseShuffleHandle            | other                 |                                                              |
  
  source code
  
  ```scala
    /**
     * Obtains a [[ShuffleHandle]] to pass to tasks.
     */
    override def registerShuffle[K, V, C](
        shuffleId: Int,
        dependency: ShuffleDependency[K, V, C]): ShuffleHandle = {
      if (SortShuffleWriter.shouldBypassMergeSort(conf, dependency)) {
        // If there are fewer than spark.shuffle.sort.bypassMergeThreshold partitions and we don't
        // need map-side aggregation, then write numPartitions files directly and just concatenate
        // them at the end. This avoids doing serialization and deserialization twice to merge
        // together the spilled files, which would happen with the normal code path. The downside is
        // having multiple files open at a time and thus more memory allocated to buffers.
        new BypassMergeSortShuffleHandle[K, V](
          shuffleId, dependency.asInstanceOf[ShuffleDependency[K, V, V]])
      } else if (SortShuffleManager.canUseSerializedShuffle(dependency)) {
        // Otherwise, try to buffer map outputs in a serialized form, since this is more efficient:
        new SerializedShuffleHandle[K, V](
          shuffleId, dependency.asInstanceOf[ShuffleDependency[K, V, V]])
      } else {
        // Otherwise, buffer map outputs in a deserialized form:
        new BaseShuffleHandle(shuffleId, dependency)
      }
    }
  ```

写磁盘：

```scala
  /** Write a bunch of records to this task's output */
  override def write(records: Iterator[Product2[K, V]]): Unit = {
    sorter = if (dep.mapSideCombine) {
      new ExternalSorter[K, V, C](
        context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)
    } else {
      // In this case we pass neither an aggregator nor an ordering to the sorter, because we don't
      // care whether the keys get sorted in each partition; that will be done on the reduce side
      // if the operation being run is sortByKey.
      new ExternalSorter[K, V, V](
        context, aggregator = None, Some(dep.partitioner), ordering = None, dep.serializer)
    }
    sorter.insertAll(records)

    // Don't bother including the time to open the merged output file in the shuffle write time,
    // because it just opens a single file, so is typically too fast to measure accurately
    // (see SPARK-3570).
    val mapOutputWriter = shuffleExecutorComponents.createMapOutputWriter(
      dep.shuffleId, mapId, dep.partitioner.numPartitions)
    sorter.writePartitionedMapOutput(dep.shuffleId, mapId, mapOutputWriter)
    val partitionLengths = mapOutputWriter.commitAllPartitions()
    mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths, mapId)
  }
```



### 5 内存管理

#### 5.1 内存分类

#### 5.2 内存的配置



