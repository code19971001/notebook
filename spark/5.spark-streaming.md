## Spark Streaming

> Spark Streaming基于spark core的准实时，微批次(时间的概念)的数据处理框架。

### 1 基本概念

#### 1.1 数据抽象 

和spark基于RDD的概念很相似，结构抽象是离散话流`DStream`。`DStream`是随时间推移而接受到的数据的序列，实际上就是对RDD的封装，在内部，每个时间区间收到的数据都作为RDD而存在。

![image-20211006095512017](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20211006095512017.png)

#### 1.2 背压机制

> 主要用来处理数据的接收和处理问题的不平衡。

在spark1.5以前的版本。如果我们想要限制Receiver的数据接收速率，我们可以设置静态配置参数`spark.streaming.receiver.maxRate`的值来实现，尽管这种方式可以限制接受速率来适配当前的处理能力，防止内存溢出，但是也会引入其他的问题：

- producer数据生产速度高于maxRate，当前集群的处理能力也高于maxRate，就会造成资源利用率下降的问题。

为了更好的协调数据接收速率与资源处理能力，新增动态控制数据接受速率来适配集群数据处理能力，即背压机制，根据JobScheduler反馈作业的执行信息来动态调整Receiver的数据接收速率。

- 参数：`spark.streaming.backpressure.enabled`来控制是否启用backpressure机制，默认值为false，不启用。

### 2 DStream入门

#### 2.1 Word Count

> 需求:使用netcat工具向9999端口不断地发送数据，通过SparkStreaming读取端口数据并统计不同单词出现的次数。

1 依赖

```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming_2.12</artifactId>
    <version>3.0.0</version>
</dependency>
```

2 代码

```scala
  def main(args: Array[String]): Unit = {
    import org.apache.spark.storage.StorageLevel
    import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
    import org.apache.spark.{SparkConf, SparkContext}
    import org.apache.spark.streaming.{Duration, Seconds, StreamingContext}
    val duration: Duration = Seconds(3)
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("word-count")
    val ssc = new StreamingContext(sparkConf, duration)
    val lines: ReceiverInputDStream[String] = ssc.socketTextStream("localhost", 9090, StorageLevel.MEMORY_ONLY)
    val words: DStream[String] = lines.flatMap(_.split(" "))
    val word2One: DStream[(String, Int)] = words.map((_, 1))
    val word2Count: DStream[(String, Int)] = word2One.reduceByKey(_ + _)
    word2Count.print()
    //启动采集器
    ssc.start()
    //等待采集器的关闭
    ssc.awaitTermination()
    ssc.stop()
  }
```

3 解析

DStream由一系列连续的的RDD来表示，每个RDD含有一段时间间隔内的数据，对数据的操作也是按照RDD为单位来进行的。，整个计算的过程是通过Spark引擎来做的。

![image-20211006112159180](https://gitee.com/code1997/blog-image/raw/master/bigdata/image-20211006112159180.png)

#### 2.2 自定义采集器

> Spark自定义采集器：继承Receiver，实现onStart and onStop方法来自定义数据源采集，可以通过整个方法来采集mysql中的数据源。

1 需求

自定义采集器.

2 代码

```scala
/**
 * 自定义采集器
 *
 * @author : code1997
 * @date : 2021/10/6 11:34
 */
object ConsumerReceiver {

  def main(args: Array[String]): Unit = {
    import org.apache.spark.SparkConf
    import org.apache.spark.streaming.dstream.ReceiverInputDStream
    import org.apache.spark.streaming.{Seconds, StreamingContext}
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("word-count")
    val ssc = new StreamingContext(sparkConf, Seconds(3))
    val message: ReceiverInputDStream[String] = ssc.receiverStream(new MyReceiver())
    message.print()
    //启动采集器
    ssc.start()
    //等待采集器的关闭
    ssc.awaitTermination()
  }

  class MyReceiver extends Receiver[String](StorageLevel.MEMORY_ONLY) {

    private var flag = true

    override def onStart(): Unit = {
      new Thread(() => {
        while (flag) {
          import scala.util.Random
          val message: String = "采集的数据：" + new Random().nextInt(10)
          println(message)
          //数据封装和转换
          store(message)
          Thread.sleep(500)
        }
      }).start()
    }
    override def onStop(): Unit = {
      flag = false
    }
  }
}
```

#### 2.3 整合Kafka

>使用新版本的directAPI，由计算的Executor来主动消费Kafka的数据，速度由自身来控制。

1 导入依赖

```xml
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming_2.12</artifactId>
    <version>3.0.0</version>
</dependency>
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming-kafka-0-10_2.12</artifactId>
    <version>3.0.0</version>
</dependency>
<dependency>
    <groupId>com.fasterxml.jackson.core</groupId>
    <artifactId>jackson-core</artifactId>
    <version>2.10.1</version>
</dependency>
```

2 代码

```scala
/**
 * 接收kafka的数据源
 *
 * @author : code1997
 * @date : 2021/10/6 12:19
 */
object KafkaStreaming {

  import scala.collection.mutable

  def main(args: Array[String]): Unit = {
    import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord}
    import org.apache.spark.SparkConf
    import org.apache.spark.streaming.dstream.InputDStream
    import org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}
    import org.apache.spark.streaming.{Seconds, StreamingContext}
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("word-count")
    val ssc = new StreamingContext(sparkConf, Seconds(3))
    //采集接待和计算节点的匹配：我们选择由框架来进行匹配.
    val kafkaParams: Map[String, Object] = Map[String, Object](
      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> "hadoop02:9092,hadoop03:9092,hadoop04:9092",
      ConsumerConfig.GROUP_ID_CONFIG -> "spark-kafka",
      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -> "org.apache.kafka.common.serialization.StringDeserializer",
      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -> "org.apache.kafka.common.serialization.StringDeserializer"
    )
    val dStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream(ssc,
      LocationStrategies.PreferConsistent,
      ConsumerStrategies.Subscribe[String, String](mutable.Set("spark-kafka"), kafkaParams))
    dStream.map(_.value()).print()
    //启动采集器
    ssc.start()
    //等待采集器的关闭
    ssc.awaitTermination()
  }
}
```

#### 2.4 DStream 转换操作

无状态转换操作：将RDD的转换操作应用到每个批次，也就是转换DStream中的每一个RDD。

有状态转换操作：有的时候我们需要在DStream中跨批次维护状态(例如流式计算中累加wordcount)。针对这种情况，updateStateByKey为我们提供了对一个状态变量的访问，用于键值形式的DStream。给一个由(键，事件)对构成的DStream，并传递一个指定如何根据新的事件更新每一个键对应状态的函数，构建出一个新的DStream，其内部数据为(键，状态)对。

#### 2.5 常见操作

##### 2.5.1 transform

transform：允许DStream上执行任意的RDD-to-RDD函数，即使这些函数并没有在DStream API中暴露出来，通过该函数可以扩展Spark API，会应该用到每一个批次的DStream。

```scala
    //map:driver端执行
    line.map(word => {
      //executor端执行
      word
    })
    //transform:driver端执行
    line.transform(rdd => {
      //driver端执行,周期性的执行
      rdd.map(
        str => {
          //executor端执行
          str
        }
      )
    })
```

为什么使用transform

- DStream的功能不完善。
- 需要代码周期性的执行。

##### 2.5.2 join

两个流之间的join需要两个流的批次大小一致，这样才可以做到同时触发计算，计算过程就是对当前批次的两个流中各自的RDD进行join，与两个RDD的join的效果相同，实际上底层使用的就是RDD的join操作。

##### 2.5.3 Windows Operations

通过设置窗口的大小和滑动窗口的间隔来动态的获取当前Streaming的允许状态，每次滑动都会触发一次计算。所有基于窗口的操作都需要两个参数：窗口的时长以及滑动步长。

- 窗口时长：计算内容的时间范围。
- 滑动步长：隔多久触发一次。

```scala
//第一个参数：窗口长度，我们的窗口长度应该是采样间隔的整数倍，防止一个采集周期的数据被裁剪开.
//第二个参数：窗口步长，默认情况下一个采集周期进行滑动，但是如果窗口长度>1倍采集周期，那么会存在重复数据.
word2One.window(Seconds(3),Seconds(3))
```

